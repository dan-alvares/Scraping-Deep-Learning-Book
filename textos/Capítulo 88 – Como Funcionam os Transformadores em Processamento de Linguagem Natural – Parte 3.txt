Conte√∫do criado por Data Science Academy
Link para acessar o conte√∫do original: https://www.deeplearningbook.com.br/como-funcionam-os-transformadores-em-processamento-de-linguagem-natural-parte-3/
//////////////////////////////
Cap√≠tulo 88 ‚Äì Como Funcionam os Transformadores em Processamento de Linguagem Natural ‚Äì Parte 3
//////////////////////////////
Vamos seguir nossa discuss√£o sobre os Transformadores com a parte mais importante, a Auto-Aten√ß√£o. Este cap√≠tulo considera que voc√™ leu os cap√≠tulos anteriores.
O que √© Auto-Aten√ß√£o?
‚ÄúA Auto-Aten√ß√£o, √†s vezes chamada de Intra-Aten√ß√£o, √© um mecanismo de aten√ß√£o que relaciona diferentes posi√ß√µes de uma √∫nica sequ√™ncia para computar uma representa√ß√£o da sequ√™ncia.‚Äù ~ Ashish Vaswani et al. do Google Brain.
A Auto-Aten√ß√£o nos permite encontrar correla√ß√µes entre diferentes palavras de entrada, indicando a estrutura sint√°tica e contextual da frase (e por isso os Transformadores, que s√£o baseados na Auto-Aten√ß√£o, s√£o muito utilizados em Processamento de Linguagem Natural).
Vamos tomar como exemplo a sequ√™ncia de entrada ‚ÄúHello, I love you‚Äù (Ol√°, Eu te amo). Uma camada de Auto-Aten√ß√£o treinada associar√° a palavra ‚Äúlove‚Äù √†s palavras ‚ÄòI‚Äù e ‚Äúyou‚Äù a um peso maior do que a palavra ‚ÄúHello‚Äù. Pela lingu√≠stica, sabemos que essas palavras compartilham uma rela√ß√£o sujeito-verbo-objeto e essa √© uma maneira intuitiva de entender o que a Auto-Aten√ß√£o ir√° capturar.

Na pr√°tica, o Transformer usa 3 representa√ß√µes diferentes: as consultas, chaves e valores da matriz de embedding. Isso pode ser feito facilmente multiplicando nossa entrada representada pela express√£o abaixo:

com 3 matrizes de peso diferentes, Wq, Wk e Wv. Em ess√™ncia, √© apenas uma multiplica√ß√£o de matrizes de embeddings. O diagrama abaixo mostra como isso funciona:

Tendo as matrizes Query (Q), Key (K) e Value (V), agora podemos aplicar a camada de Auto-Aten√ß√£o como:

No artigo original dos Transformadores, a aten√ß√£o do produto escalonado foi escolhida como uma fun√ß√£o de pontua√ß√£o para representar a correla√ß√£o entre duas palavras (o peso da aten√ß√£o). Observe que tamb√©m podemos utilizar outra fun√ß√£o de similaridade. A raiz quadrada de d_k na f√≥rmula acima age simplesmente como um fator de escala para garantir que os vetores n√£o explodam.
Este termo simplesmente encontra a similaridade da consulta de pesquisa com uma entrada em um banco de dados. Finalmente, aplicamos uma fun√ß√£o softmax para obter os pesos finais de aten√ß√£o como uma distribui√ß√£o de probabilidade (da mesma forma que fazemos em diversos modelos de Deep Learning para classifica√ß√£o).
Lembre-se de que distinguimos as Chaves (K) dos Valores (V) como representa√ß√µes distintas. Assim, a representa√ß√£o final √© a matriz de Auto-Aten√ß√£o (a express√£o com softmax na f√≥rmula acima) multiplicada pela matriz de valores V.
Podemos pensar na matriz de Auto-Aten√ß√£o como para onde olhar e na matriz de valor como o que eu realmente quero obter.
E aqui h√° um detalhes sobre a similaridade do vetor:
Primeiro, temos matrizes em vez de vetores e, como resultado, multiplica√ß√µes de matrizes. Em segundo lugar, n√£o diminu√≠mos pela magnitude do vetor, mas pelo tamanho da matriz (d_k), que √© o n√∫mero de palavras em uma frase! E o tamanho da frase varia. üôÇ
O que far√≠amos a seguir?
Normaliza√ß√£o e conex√µes de salto curto, semelhantes ao processamento de um tensor ap√≥s convolu√ß√£o (CNN) ou recorr√™ncia (RNN). Mas deixamos isso para o pr√≥ximo cap√≠tulo. At√© l√°.
Refer√™ncias:
Attention Is All You Need
Deep Learning II
Processamento de Linguagem Natural
The Illustrated Transformer
Understanding Attention In Deep Learning
How Transformers work in deep learning and NLP: an intuitive introduction
